{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e5ebd0dc-4ecb-40ae-803e-67e99e272eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import math\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "2d8fcb57-5c61-405a-b021-f917ddbc0c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, d_model, attention_dim, max_sequence_length, dropout):\n",
    "        super().__init__()\n",
    "        self.qw = nn.Linear(d_model, attention_dim)\n",
    "\n",
    "        self.qkvw = nn.Linear(attention_dim, attention_dim)\n",
    "\n",
    "        self.dropout = nn.Dropout(p = dropout)\n",
    "\n",
    "        mask = torch.tril(torch.ones(max_sequence_length, max_sequence_length))\n",
    "\n",
    "        self.register_buffer(\"mask\", mask)\n",
    "\n",
    "    def forward(self, x, k, v, inference = False):\n",
    "\n",
    "        q = self.qw(x)\n",
    "\n",
    "        batch_size, seq_length, attention_dim = k.shape\n",
    "\n",
    "        new_length = q.shape[1]\n",
    "\n",
    "        past_length = seq_length - new_length\n",
    "        \n",
    "        qk = q @ torch.transpose(k,-1,-2)\n",
    "\n",
    "        if inference == True:\n",
    "            mask = self.mask[past_length:seq_length, :seq_length]\n",
    "            \n",
    "        else:\n",
    "            mask = self.mask[:seq_length, :seq_length]\n",
    "\n",
    "        qk = qk.masked_fill(mask == 0, float(\"-inf\"))\n",
    "\n",
    "        qk = qk / math.sqrt(attention_dim)\n",
    "\n",
    "        qk = F.softmax(qk, dim = -1)\n",
    "\n",
    "        qk = self.dropout(qk)\n",
    "\n",
    "        qkv = qk @ v\n",
    "\n",
    "        qkv = self.qkvw(qkv)\n",
    "\n",
    "        return qkv\n",
    "        \n",
    "\n",
    "\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "9d0733cd-ecd5-4363-b8da-96d003a87e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, max_sequence_length, heads, dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        assert d_model % heads == 0, \"d_model should be perfectly divisible by heads\"\n",
    "\n",
    "        attention_dim = int(d_model // heads)\n",
    "\n",
    "        self.kw = nn.Linear(d_model, attention_dim)\n",
    "        self.vw = nn.Linear(d_model, attention_dim)\n",
    "        \n",
    "        self.attention_heads = nn.ModuleList([Attention(d_model, attention_dim, max_sequence_length, dropout) for _ in range(heads)])\n",
    "        self.kv_cache = (None,None)\n",
    "\n",
    "\n",
    "    def forward(self, x, inference = False):\n",
    "\n",
    "        k = self.kw(x)\n",
    "        v = self.vw(x)\n",
    "        if inference == True:\n",
    "            k_prev, v_prev = self.kv_cache\n",
    "            if k_prev is not None:\n",
    "                k = torch.cat([k_prev, k], dim = 1)\n",
    "                v = torch.cat([v_prev, v], dim = 1)\n",
    "            self.kv_cache = (k,v)\n",
    "\n",
    "        mha = [attention(x,k,v, inference) for attention in self.attention_heads]\n",
    "        mha = torch.cat(mha, -1)\n",
    "\n",
    "        return mha\n",
    "            \n",
    "        \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c440baf4-4bfc-4efb-a56f-c6fb42d61139",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.8173, 0.0368, 0.3520, 0.0292, 0.4106, 0.2075],\n",
       "         [0.6018, 0.9990, 0.0660, 0.8565, 0.5285, 0.7071]],\n",
       "\n",
       "        [[0.7028, 0.4620, 0.4774, 0.1843, 0.2528, 0.5330],\n",
       "         [0.2417, 0.5211, 0.0090, 0.7061, 0.4615, 0.9414]],\n",
       "\n",
       "        [[0.1067, 0.7722, 0.1677, 0.8867, 0.3147, 0.4183],\n",
       "         [0.0540, 0.6101, 0.3978, 0.9347, 0.1291, 0.5395]]])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rand = torch.rand(3, 2,6)\n",
    "rand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "54e729b4-5f01-402b-91dd-0b16d2500f12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultiHeadAttention(\n",
       "  (kw): Linear(in_features=6, out_features=2, bias=True)\n",
       "  (vw): Linear(in_features=6, out_features=2, bias=True)\n",
       "  (attention_heads): ModuleList(\n",
       "    (0-2): 3 x Attention(\n",
       "      (qw): Linear(in_features=6, out_features=2, bias=True)\n",
       "      (qkvw): Linear(in_features=2, out_features=2, bias=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mha = MultiHeadAttention(6, 10, 3, 0.1)\n",
    "mha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "7b469de6-1c9a-439f-a0a1-4139b464a8ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.3353,  0.6073, -0.2341, -0.2125,  0.0043,  0.2958],\n",
       "         [ 0.2257,  0.5095, -0.4416,  0.0356, -0.4351,  0.3016]],\n",
       "\n",
       "        [[ 0.2523,  0.5537, -0.2957, -0.1468, -0.1158,  0.3171],\n",
       "         [ 0.1761,  0.4954, -0.3627, -0.0784, -0.2791,  0.2574]],\n",
       "\n",
       "        [[-0.0721,  0.5081, -0.4360,  0.0534, -0.4521,  0.2744],\n",
       "         [-0.0627,  0.5083, -0.4326,  0.0478, -0.4433,  0.2764]]],\n",
       "       grad_fn=<CatBackward0>)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mha(rand, inference = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b2b95a6-f05a-48e2-95ee-9e515ad39097",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a47c73f-317e-4336-ba50-d71f2d2d965e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base]",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
